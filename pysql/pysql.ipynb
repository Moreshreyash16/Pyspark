{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spark-SQL</h2>\n",
    "\n",
    "cpu logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 14:58:25 WARN Utils: Your hostname, ShreyashUbuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/07/21 14:58:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hdoop/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/07/21 14:58:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName(\"spark sql\").config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/mysql-connector-j-8.0.33.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('hdfs://localhost:9000/pysql/*.csv',header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 14:59:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Idle Computers From Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|           user_name|max_idle_time|\n",
      "+--------------------+-------------+\n",
      "|deepshukla292@gma...|       141300|\n",
      "|markfernandes66@g...|        85500|\n",
      "|sharlawar77@gmail...|        84300|\n",
      "|rahilstar11@gmail...|        83400|\n",
      "|bhagyashrichalke2...|        83400|\n",
      "|salinabodale73@gm...|        82500|\n",
      "|damodharn21@gmail...|        80100|\n",
      "|  iamnzm@outlook.com|        75600|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_idle=spark.sql(\"select user_name,MAX(idle_time) AS max_idle_time FROM (SELECT user_name,DateTime,UNIX_TIMESTAMP(DateTime)-UNIX_TIMESTAMP(LAG(DateTime) OVER (PARTITION BY user_name ORDER BY DateTime)) AS idle_time FROM df WHERE keyboard='0.0' and mouse='0.0') subquery GROUP BY user_name ORDER BY max_idle_time DESC\")\n",
    "max_idle.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Configure the connection properties \n",
    "from py4j.protocol import Py4JJavaError\n",
    "database_url = \"jdbc:mysql://localhost:3306/pyspark\"\n",
    "table_name = \"idle_user\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"Admin@123\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Write the DataFrame to the existing MySQL database table (append mode)\n",
    "max_idle.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", database_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"]) \\\n",
    "    .option(\"driver\", properties[\"driver\"]) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Users with lowest number of average hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           user_name|         avg_hours|\n",
      "+--------------------+------------------+\n",
      "|  iamnzm@outlook.com|13.785016286644952|\n",
      "|markfernandes66@g...|14.100393700787402|\n",
      "|deepshukla292@gma...|14.130973451327433|\n",
      "|sharlawar77@gmail...|14.205172413793104|\n",
      "|damodharn21@gmail...|14.312252964426877|\n",
      "|bhagyashrichalke2...|14.352697095435685|\n",
      "|rahilstar11@gmail...| 14.37568058076225|\n",
      "|salinabodale73@gm...|14.576449912126538|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lowest_avg_hours=spark.sql(\"\"\"  SELECT user_name,AVG(hour(DateTime)) AS avg_hours\n",
    "                                    FROM df \n",
    "                                    GROUP BY user_name \n",
    "                                    ORDER BY avg_hours \"\"\")\n",
    "lowest_avg_hours.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Configure the connection properties \n",
    "from py4j.protocol import Py4JJavaError\n",
    "database_url = \"jdbc:mysql://localhost:3306/pyspark\"\n",
    "table_name = \"lowest_avg\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"Admin@123\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Write the DataFrame to the existing MySQL database table (append mode)\n",
    "lowest_avg_hours.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", database_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"]) \\\n",
    "    .option(\"driver\", properties[\"driver\"]) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Users with highest number of average hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           user_name|         avg_hours|\n",
      "+--------------------+------------------+\n",
      "|salinabodale73@gm...|14.576449912126538|\n",
      "|rahilstar11@gmail...| 14.37568058076225|\n",
      "|bhagyashrichalke2...|14.352697095435685|\n",
      "|damodharn21@gmail...|14.312252964426877|\n",
      "|sharlawar77@gmail...|14.205172413793104|\n",
      "|deepshukla292@gma...|14.130973451327433|\n",
      "|markfernandes66@g...|14.100393700787402|\n",
      "|  iamnzm@outlook.com|13.785016286644952|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highest_avg_hours=spark.sql(\"\"\"SELECT user_name,AVG(hour(DateTime)) AS avg_hours\n",
    "                                    FROM df\n",
    "                                    GROUP BY user_name \n",
    "                                    ORDER BY avg_hours DESC\"\"\")\n",
    "highest_avg_hours.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Configure the connection properties \n",
    "from py4j.protocol import Py4JJavaError\n",
    "database_url = \"jdbc:mysql://localhost:3306/pyspark\"\n",
    "table_name = \"highest_avg\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"Admin@123\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Write the DataFrame to the existing MySQL database table (append mode)\n",
    "highest_avg_hours.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", database_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"]) \\\n",
    "    .option(\"driver\", properties[\"driver\"]) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Users with late comings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|           user_name|           DateTime|\n",
      "+--------------------+-------------------+\n",
      "|  iamnzm@outlook.com|2019-09-19 08:40:02|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:45:02|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:50:01|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:55:01|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:00:01|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:05:01|\n",
      "|deepshukla292@gma...|2019-09-19 09:05:01|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:10:01|\n",
      "|deepshukla292@gma...|2019-09-19 09:10:01|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:15:02|\n",
      "|deepshukla292@gma...|2019-09-19 09:15:02|\n",
      "|markfernandes66@g...|2019-09-19 09:15:01|\n",
      "|markfernandes66@g...|2019-09-19 09:10:01|\n",
      "|markfernandes66@g...|2019-09-19 09:20:01|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:20:02|\n",
      "|deepshukla292@gma...|2019-09-19 09:20:02|\n",
      "|markfernandes66@g...|2019-09-19 09:25:01|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:25:01|\n",
      "|deepshukla292@gma...|2019-09-19 09:25:01|\n",
      "|markfernandes66@g...|2019-09-19 09:30:01|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select user_name,DateTime\n",
    "           FROM df \n",
    "           WHERE datetime>'2019-09-16 13:00:00'\n",
    "           \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 15:02:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 10:=================================================>    (185 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|           user_name|late_count|user_rank|\n",
      "+--------------------+----------+---------+\n",
      "|  iamnzm@outlook.com|       576|        1|\n",
      "|salinabodale73@gm...|       569|        2|\n",
      "|sharlawar77@gmail...|       568|        3|\n",
      "|rahilstar11@gmail...|       551|        4|\n",
      "|deepshukla292@gma...|       547|        5|\n",
      "|markfernandes66@g...|       496|        6|\n",
      "|bhagyashrichalke2...|       482|        7|\n",
      "|damodharn21@gmail...|       253|        8|\n",
      "+--------------------+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "late_users_df = spark.sql(\"\"\"\n",
    "    SELECT user_name, COUNT(*) AS late_count,\n",
    "           RANK() OVER (ORDER BY COUNT(*) DESC) AS user_rank\n",
    "    FROM df\n",
    "    WHERE hour(DateTime) > 9 OR (hour(DateTime) = 9 AND minute(DateTime) > 30)\n",
    "    GROUP BY user_name\n",
    "    ORDER BY late_count DESC\n",
    "\"\"\")\n",
    "\n",
    "# Display the result\n",
    "late_users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 15:02:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Configure the connection properties \n",
    "from py4j.protocol import Py4JJavaError\n",
    "database_url = \"jdbc:mysql://localhost:3306/pyspark\"\n",
    "table_name = \"late_commers\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"Admin@123\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Write the DataFrame to the existing MySQL database table (append mode)\n",
    "late_users_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", database_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"]) \\\n",
    "    .option(\"driver\", properties[\"driver\"]) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
